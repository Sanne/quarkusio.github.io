---
layout: post
title: 'Exploring Project Leyden'
date: 2024-07-17
tags: leyden
synopsis: 'Project Leyden: exploring its potential for Quarkus users.'
author: leydenquarkus
---
:imagesdir: /assets/images/posts/leyden

You might have heard of https://openjdk.org/projects/leyden/[Project Leyden]: an initiative within the OpenJDK project with very ambitious goals.

When casually namedropping "I'm looking into Project Leyden" with some fellow Java developers these days, I would usually spot some off guard thinking: "Leyden..?", to then offer a hasty save "Ah, right, the project to improve startup times".

Which is not wrong, as indeed startup times improvements are one of its goals, yet we believe the other stated goals of the project offer even greater potential for our favourite platform, and its users.

== What is Leyden?

Project Leyden is an initiative from the OpenJDK team. It is an ongoing experiment that is currently being developed by the joint effort of teams from different companies.

[quote]
____
The primary goal of this Project is to improve the startup time, time to peak performance, and footprint of Java programs.

-- Project Leyden, first thing on its project page
____

Leyden is a general umbrella project to address slow startup and large footprint. To keep costs down, in all forms such as energy consumption, required hardware resources, and indeed money, it's indeed useful to keep bootstrap times reasonably low, but it's even more effective to reduce the time to peak performance, such as the time it takes for the JVM to "warm up", and reducing the footprint of our applications, such as total memory, has very direct impact.

Note that the project is evolving rapidly: some of the things explained in this article are evolving while this is written. If you plan on getting involved at a more technical level, follow the development in Jira and the https://mail.openjdk.org/mailman/listinfo/leyden-dev[Leyden mailing list].

=== Why it’s interesting to Quarkus

From a Quarkus perspective, we've done a fair job on all such metrics but we're constantly on the lookout to improve. That's why Project Leyden got our attention. We're already working with our collagues from the OpenJDK team at Red Hat, who are directly involved in implementing Leyden with the wider OpenJDK group: this blog post today is a collaboration among engineers from different teams.

Although Quarkus is already doing a lot of work during the Ahead of Time phase to speed up warmup and response time, the enhancements that Leyden is bringing to the table are more related to how the JVM behaves. Complementing both approaches, the advantages we can expect from the combination of Quarkus and Leyden are beyond anything you can find with either of them separated.

Since the potential for such technological collaboration is strong, the Quarkus and OpenJDK teams are working together on various prototypes and anyone in the Quarkus community would be very welcome to join as well.

== Refresher on JVM's bootstrap process

To better understand the scope of the potential improvements, we need to take a step back and discuss how the JVM works today, especially how our application is started and iteratively evolves from interpreting our bytecode to its highest performance mode: running native code which is highly optimized, adapted to the particular hardware, the configuration of the day, and the specific workloads it's been asked to perform. No other runtime is able to match the JVM on this.

By default, as it boots, Java is a lazy interpreted language. A Java runtime does not directly run Java source code. The content of our JAR file is not executable machine code, but Java bytecode generated from Java source code using the javac compiler. 

A key feature of bytecode is portability, encoding the structure of Java classes and operation of their methods in a machine and operating-system independent format. A Java runtime obeys the type information in the bytecode when laying out Java objects. 

Execution of a method normally involves interpreting the operations in the method bytecode, although a runtime may also choose to compile method bytecode to equivalent machine code and execute the latter directly. 

The unit of bytecode is a class file, which models a single class. The Java runtime provides a host of utility and runtime management classes, as class files embedded in either system jars or jmod files. Applications supplement this with their own class files, usually by appending jars to the classpath or module path.

Bytecode is provided class-at-a-time to allow the runtime to load classes _lazily_: i.e. the runtime will only lookup, verify and consume a class file when that class's definition is required to proceed with execution. 

Lazy loading is what allows Java to be a dynamic language -- i.e. one where the code that is included in the program can be decided at runtime. That might include loading classes from jars selected at runtime or, possibly, loaded from the network. Alternatively, it might include generating class bytecode at runtime, as is done with proxy classes or service provider auxiliary classes.

What is less well known is that many other operations performed by the JVM are done lazily during runtime or 'Just In Time'. 

=== Just in Time (JIT) and Ahead of Time (AOT)

An alternative to doing things 'Just in Time' (JIT) is to do them 'Ahead Of Time' (AOT). For example, Graal's Native Image runtime loads the bytecode of every single class needed by an application, including JDK runtime classes, 'Ahead Of Time' i.e. at image build time. It uses the type and method information encoded in that bytecode to 'Ahead Of Time' compile a complete program that includes code for every method that might possibly be executed by the application. 

Graal Native Image lies at one extreme, everything is done AOT, and the traditional Java runtime model lies at the other extreme, everything is done JIT. However, it is actually possible to mix and match AOT and JIT models of execution in one runtime: that is a goal of the first EA release of project Leyden.

image::AoT_vs_JiT.svg[Compilation work diagram,float="right",align="center"]
 
The JVM gets the bytecode to run the application, usually from a JAR file format. When the JVM executes a new bytecode, the JVM will check, method by method, if it is already compiled.
While processing this bytecode, it may happen that other methods are invoked, or some class field gets read or written, that hasn't been processed before.
When this happens, the JVM doesn't just attempt to load the relevant class. It checks in the bytecode for a matching field or method of the correct type and attaches linkage information to the loaded class in memory.

This linkage makes sure that any subsequent execution can proceed efficiently without the need to repeat the lookup the next time. This is why, after some warm up time, the application runs faster: it has everything needed already linked and compiled.

== Profiling and Training Runs

This lazy approach results in the exact same bytecode being loaded and parsed for many classes every run, the exact same linkage being established every run, the exact same compilation, profiling and recompilation being attempted at every run. This can noticeably slow down JDK startup, application startup and application warm up (time to peak running).

We could speed up startup and, more crucially, warm up time if we do some of these lazy actions at an earlier stage.

Compiling code for peak performance also requires quite some resources, so performing this work ahead of time can also save precious CPU cycles during the application bootstrap, and can manifest in substantial memory savings as well.

But there are some limitations on what we can optimise before runtime just by examining the source code. For example, extensive use of reflection prevents the compiler from predicting which symbols will be loaded, linked, and most used at runtime.

=== Class Data Sharing (CDS)

Indeed, this is not a wholly new idea as far as the OpenJDK runtime is concerned. OpenJDK has supported a mixed AOT/JIT class loading model for years with CDS. The observation that led to https://docs.oracle.com/en/java/javase/21/vm/class-data-sharing.html[Class Data Sharing (CDS)] being proposed was that most applications load the same classes every time they run, both JDK classes during JDK bootstrap and application classes during application startup and warmup.

Loading requires locating a class bytecode file, possibly calling out to a Java ClassLoader, parsing the bytecode then building a JVM-internal model of the class.
This internal model unpacks the information packed into the bytecode into a format that enables fast interpreted or compiled execution. If this loading work could be done once and the results efficiently reused on subsequent runs, then that would save time during startup and warm up.

Initially CDS optimized this process for a large set of core JDK classes. It worked by running the JVM and dumping a class model for all classes loaded during startup into an archive file laid out in memory format. The resulting JDK module, class, field, and method graph can then be quickly remapped into memory next time the JVM runs. 

Loading a class that is present in the archive involves a simple lookup in the AOT class model. Loading a class not present in the archive requires the normal JIT steps of bytecode lookup, parsing and unpacking. Subsequent improvements to CDS allowed application classes also to be stored in the CDS archive at the end of a short application training run.

A CDS archive for JDK classes has been built into the JVM from JDK17 onwards, reducing JDK startup time. This same mixed model AOT/JIT model provides significant improvements to application startup and warmup times, depending on how well the training run exercises application code. So, selective JIT vs AOT operation is not some new thing.

The goal of Project Leyden is extending the AOT vs JIT trade-off from class loading (as done by CDS) to other JIT operations in the JVM, the lazy linking that normally happens during interpreted execution and the lazy compilation and recompilation that happens when methods have been executed enough times to justify the cost of compilation.

=== Loading and Linking in Detail

The loading and linkage of classes is an important step in the warm up of the application because it involves searching through the whole classpath for all classes and objects referenced by the bytecode the JVM is going to run. By default, this is done as a lazy operation because loading and linking all existing classes in the classpath would not only require a bigger memory footprint, but also a bigger warm up time. This is why the JVM only compiles and links the bytecode that is going to be used.

This is a process that Quarkus already speeds up by, among other strategies, aggressively reducing the set of classes included in the classpath, so the search for matches is faster. But it is still a heavy operation that is difficult to execute ahead of time, before we know what is going to be run and how.

Leyden is moving some of such linking processes to AOT by using the CDS trained pre-linked classes archive.
The work in progress the Leyden team is working on next regarding caching is saving and restoring compiled code.

Remember that the training run enables some of the loading/linking/compilation to be done AOT but that anything not trained for will still be performed via the regular JIT process: the AOT approach is not required to be applied comprehensively, so that the JVM can fallback to the regular loading system for the use cases which can not benefit from AOT processing.


=== How does Loading and Linking work?

In the JVM, loading and linking are two independent steps which may or may not happen together:

 . When a class is loaded for the first time, the JVM models internally each class. This includes its methods and its fields. This model gets loaded in memory, where other classes and objects can have access to it.

. Then the JVM resolves references from some loaded class (or its method bytecode) to other classes (and their methods and fields) or to constant (String or Class) objects on the heap. In the bytecode, these references are done by symbolic name i.e. UTF8 strings. But in the internal model used to execute the code, those links have to be established explicitly, using memory pointers to actual memory positions.

So, that means link resolution involves traversing the class base, potentially requiring the load of a referenced class, or the creation of a referenced String or Class object on the heap memory. This second step mostly happens when bytecode gets executed. 

In a few cases it happens as a side effect of the load. For example, super classes are loaded and linked immediately when the class gets loaded. The results are cached in an internal model of the constant pool, replacing a symbol entry (UTF8 String) with a pointer and updating the tag for the entry. 

At the same time, all details of where a field is located or how to enter a method (interpreter or compiled entry address) are cached in the constant pool cache. These latter details are needed in order to access fields or call methods from the interpreter. They are also used as inputs to the compiler when it tries to compile a method.

=== Runtime Optimisation

Another lazy operation the JVM performs is JIT (runtime) compilation. Method bytecode is normally interpreted, but the JVM will lazily translate bytecode to equivalent machine code. It performs this compilation task selectively, only bothering to compile methods that have been invoked quite a few times. 

JiT will also lazily upgrade compiled code after it has been executed very many times, using a different 'tier' or level of compilation:

 . An initial tier 1 compile runs quickly, generating code that is only lightly optimised using profile information gathered during interpretation. 
 . A tier 2 recompile will instrument the code to track more details about control flow. 
 . Tier 3 compilation adds further instrumentation that records many more details about what gets executed, including with what type of values. 
 . Finally a tier 4 compilation uses the gathered profile information to perform a great deal of optimization. 

This final stage of compilation can take a very long time so compilation above tier 1 only happens for a small subset of very frequently executed methods.

Peak optimization is reached when most of the code running is compiled at the highest tier.

Leyden premain addresses the startup problem by caching loaded class info, class linkage, and compilation profile during a training run so it can reuse them to actively populate, link, and compile code in a production run. Leyden is extending CDS to add the extra stuff to the mix.

== Current status of Project Leyden

There are already experimental https://jdk.java.net/leyden/[early-access builds of Leyden] that can be tested based on https://openjdk.org/jeps/8315737[this draft JEP about Ahead-of-Time Class Linking]. With the https://www.youtube.com/watch?v=lnth19Kf-x0[Leyden Project], the training run idea has been extended to a wider range of data structures, creating the Cache Data Store(CDS). Now the training data contains:

 - Class file events with historical data (Classes loaded and linked, Compilations)
 - Resolution of API points and indy (stored in constant pool images in the CDS archive). If you have lambdas in your code, they are captured here.
 - Execution profiles and some compiled native code (all tiers)

This new CDS implementation not only tracks which classes to load, but it also saves the interrelationships that link classes together. During runtime, the JVM will know the estimated final size of a class, allowing it to calculate in AoT time locations of fields and methods. This is useful because we can prepare other classes that call those fields and methods with the appropriate pointer instead of having to wait until runtime and make the HotSpot calculate the memory pointer on the fly.

=== Some known limitations

This is an experimental project being developed by multiple teams with different approaches and focuses. Limitations explained here are being worked on at the time of writing this blog post.

One of the main issues is that functionality is currently only available for x86_64 and AArch64 architectures at the moment. 

Also, current developments rely on a flat classpath. If the application is using custom classloaders, then it may not benefit as much as it could as it may miss caching many classes. 

Same happens if the application is intensively using reflection. Quarkus avoids reflection whenever possible, preferring to resolve reflective calls at build time as well - so there’s a nice synergy right there. 

However Quarkus in “fast-jar” mode, which is the default packaging mode, will use a custom classloader which currently would get in the way of some Leyden optimisations. One could use a different packaging mode in Quarkus to get more prominent benefits from Leyden, but doing so would disable other Quarkus optimisations, so the comparison wouldn’t be entirely fair today.

The focus on these first early releases has been on bootstrap times. There are measurable, significant startup time improvements, due to AoT loading and linking. In some cases, these improvements on startup time have worsened the memory footprint of some applications. That’s an already known issue that is being worked on, and the expected outcome is to improve memory footprint as well, so we would suggest not worrying too much about total memory consumption at this stage.

Since the CDS archives include machine specific optimisations such as the native code generated by the C2 compiler, the training run and the production run must be done on the same type of hardware and JDK versions; it also requires using the same JAR-based classpaths and the same command line options. 

Although you can use a different Main class for running the application, maybe a test class that simulates real usage.

=== What is on the roadmap for Leyden?

There’s still work to be done regarding classes that can’t be loaded and linked in AoT with the current implementation. For example, classes loaded using a user-defined class loader. There’s also room to improve the way the training runs are made, maybe allowing the user to tweak the results to influence decisions.

Currently, the https://bugs.openjdk.org/browse/JDK-8326035[Z Garbage Collector] does not support CDS object archiving. There is an active effort to make sure all Garbage Collectors are compatible with these enhancements.

There are also other things planned in the roadmap for Leyden, like adding condensers. https://openjdk.org/projects/leyden/notes/03-toward-condensers[Condensers] will be composable transformers of the source code in AoT that modify the source code optimising it. Each developer will be able to define a pipeline of condensers that improves their source code before compiling it into Bytecode; this is very interesting to the Quarkus team but condensers aren’t available yet

The OpenJDK team is working on adding a more complete code cache to the CDS to avoid that first compilation for trained data, by just loading the compiled code directly from the cache; our colleagues from Red Hat’s OpenJDK team are directly involved in implementing this. This could include, among others, auxiliary code used to interface compiled code to runtime, interpreter or other compiled runtimes.

== How to play with it 

The first step would be to install one of the early Leden builds that you can find in https://jdk.java.net/leyden/

Make sure that you have installed it correctly by running the following command:

[source, console]
----
$ java --version
openjdk 24-leydenpremain 2025-03-18
OpenJDK Runtime Environment (build 24-leydenpremain+2-8)
OpenJDK 64-Bit Server VM (build 24-leydenpremain+2-8, mixed mode, sharing)
----

Go to the application you want to test Leyden with and make a first training run:

[source, console]
----
$ java -XX:CacheDataStore=archive.cds -jar $YOUR_JAR_FILE
----

This will generate the archive files with all the profiling information needed to speed up the production run.

Now that we have them, we can run our application using the Leyden enhancements:

[source, console]
----
$ java -XX:CacheDataStore=archive.cds -XX:+AOTClassLinking -jar $YOUR_JAR_FILE
----

== Potentially needed workarounds

Since it’s early days for the Leyden project, there are some known limitations. The following instructions shouldn’t be necessary for the final versions but you might need them today.

=== Force the use of G1GC

To benefit from the natively compiled code in CDS archives, the garbage collector used at runtime needs to match the same garbage collector used when you recorded the CDS archives. 

Remember that the JVM’s default choice of garbage collector is based on ergonomics; normally this is nice but it can cause some confusion in this case; for example if you build on a large server it will pick G1GC by default, but then when you run the application on a server with constrained memory it would, by default, pick SerialGC.

To avoid this it’s best to pick a garbage collector explicitly; and since several CDS related optimisations today only apply to G1, let’s enforce the use of G1GC.

Force using G1GC:

[source, console]
----
-XX:+UseG1GC
----

N.B. you need to use this consistently on both the process generating the CDS archives and the runtime.

=== Force the G1 Region sizes

As identified and reported by the Quarkus team to our colleagues working on Project Leyden, beyond enforcing a specific garbage collector one should also ensure that the code stored in CDS archives is being generated with the same heap region sizes as what’s going to be used at runtime, or one risks segmentation faults caused by it wrongly identifying regions.
See https://bugs.openjdk.org/browse/JDK-8335440 for details, or simply set:

Configure G1HeapRegionSize explicitly:

[source, console]
----
-XX:G1HeapRegionSize=1048576
----

N.B. you need to use this consistently on both the process generating the CDS archives and the runtime.

=== Failure to terminate in containers

This issue has already been resolved, but in case you’re using an older version of project Leyden and it fails to exit on regular container termination, you might be affected by https://bugs.openjdk.org/browse/JDK-8333794[JDK-8333794].

Workaround for JDK-8333794:

[source, console]
----
-Djdk.console=java.basebroken
----

== Will Leyden replace GraalVM's native-image capabilities?

The short answer is no.

If you want the absolute smallest footprint and ensure that absolutely no "dynamic" adaptations happen at runtime, GraalVM native images are the way to go. Just think about it: to support the dynamic aspects that the JVM normally provides,
even in very minimal form, you would need some code which is able to perform this work, and some memory and some computational resources to run such code and adapt your runtime safely; this is a complex feature and will never be completely free, even in the case Leyden evolved significantly beyond the current plans.

The architecture of Quarkus enables developers to define an application in strict "closed world" style, and this approach works extremely well in combination with GraalVM native images, but this design works indeed very well on the bigger, dynamic JVMs as well.

The ability that Quarkus offers to created a closed world application doesn't imply that you should necessarily be doing so; in fact there are many applications which could benefit from a bit more dynamism, a bit more runtime configurability or auto-adaptability, and Quarkus also allows to create such applications while still benefitting from very substantial efficiency improvements over competing architectures, and even competing runtimes and languages.

We're very excited by Project Leyden as it allows to substantially improve bootstrap times, warmup times, and overall costs even for the "regular" JVM, so retaining all the benefits of a dynamic runtime and an adaptative JIT compiler, and this will be a fantastic option for all those applications for which a fully AOT native image might not be suitable: you'll get some of the benefits from native-image (not all of them) but essentially for free, at no drawbacks.

We also hope it will bring better defined semantics in regards to running certain phases “ahead of time” (or later); there is a very interesting read on this topic by Mark Reinhold: “Selectively Shifting and Constraining Computation” ; from a perspective of Quarkus extensions maintainers, we can confirm that this would be very welcome, and also improve the quality and maintainability of applications compiled with GraalVM native-image(s).

For these reasons, Quarkus will definitely not deprecate support for native images; it's more plausible that, eventually, the "full JVM" will always be benefitting from Leyden powered improvements, and as usual we'll work to make these benefits work in synergy with our architecture, and at minimal effort for you all.

It's a great time to be a Java developer!


== How can I make sure this will work for me?

The best way to make sure your application benefits from Leyden is to start experimenting early, be involved in the development. It would be great to add real-world feedback from a perspective of Quarkus users.

If you spend some time testing your application with the https://jdk.java.net/leyden/[early-access builds of Leyden], and reporting any https://bugs.openjdk.org/browse/JDK-8335735?jql=issuetype%20%3D%20Bug%20AND%20status%20%3D%20Open%20AND%20labels%20%3D%20leyden[bugs] or weird behaviour; you will ensure the developers will take your specificities into account.

The OpenJDK issue tracker isn’t open to everyone, but you’re also very welcome to provide feedback on our https://quarkus.io/discussion/[Quarkus channels]; we can then relay any improvement ideas to our colleagues who are directly working on project Leyden. You can also use the https://mail.openjdk.org/mailman/listinfo/leyden-dev[Leyden mailing list].

